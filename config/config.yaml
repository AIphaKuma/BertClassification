# ğŸ“ Configuration globale du projet
task: "classification"  # On entraÃ®ne un modÃ¨le de classification pour QA sur les capitales

# ğŸ“Œ ModÃ¨le utilisÃ©
model_name: "distilbert-base-uncased"  # Utilisation d'un modÃ¨le lÃ©ger
tokenizer_type: "bert"

# ğŸ”¥ HyperparamÃ¨tres du modÃ¨le
learning_rate: 0.00003  # Un learning rate un peu plus bas
num_epochs: 15  # Augmenter le nombre d'Ã©poques
batch_size: 16  # Augmenter la stabilitÃ©
seq_length: 50  # Longueur des sÃ©quences d'entrÃ©e max
max_seq_len: 512  # Taille maximale des tokens d'entrÃ©e

# ğŸ“‚ DonnÃ©es
dataset_name: "data/dataset_bert.csv"  # Dataset CSV contenant les questions et les capitales
train_split: 0.8  # 80% des donnÃ©es pour l'entraÃ®nement
val_split: 0.2  # 20% pour la validation

# ğŸ”„ Chemins de sauvegarde
model_save_path: "models/distilbert-capital-classification"
tokenizer_save_path: "models/tokenizer_distilbert"
dataset_save_path: "data/processed_dataset.csv"  # Si on veut prÃ©-traiter le dataset avant l'entraÃ®nement

# ğŸ›ï¸ ParamÃ¨tres de gÃ©nÃ©ration (si utilisÃ© en inference)
generation_length: 100
temperature: 0.8
top_k: 10
top_p: 0.9